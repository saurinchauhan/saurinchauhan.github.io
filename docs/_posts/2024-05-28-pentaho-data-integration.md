---
layout: posts
title:  Pentaho Data Integration (PDI)
date:   2024-05-28 23:15:37 +0530
categories: tech
author: Saurin Chauhan
---


PDI short for Pentaho Data Integration tool's main use as the name suggests is for data integration, with PDI one can create various Transformations and Jobs and schedule them to run on desired frequency. 

It's a very useful tool in developing transformations which can process large size of data inputs and perform complex calculation on the data.

PDI provides a very interactive UI to develop the transformations where you have to select various input, transform and output components and map them with one another. It provides a UI where you can develop the transformation as **workflow diagram**.

Once mastered you can very quickly develop a transformation, which will take a lot of time otherwise if you have to develop using any programming language.

Let's talk about few terminologies we used.

**Transformations:** These files run its nodes in parallel, if you have multiple input sources in a transformations it will start consuming data from all input sources at once when the transformation job starts and all the calculations and data processing is again done in parallel. How to pick the node for processing is taken care by PDI itself.
Transformation files are created with .ktr extension.

**Jobs:** These files run the nodes in sequence which means from the start when a node finishes its processing then only it moves to next node and so on for the other nodes. In jobs you can use already created **Transformations**.

In both Transformations and Jobs you can define data flow based on various conditions and send the data to different output targets such as files or another relational databases.


PDI has various tools such as Spoon, Pan and Kitchen to create and run the transformations and jobs.

**Spoon** is a desktop client to create the transformations and jobs, it comes with huge number of options for setting up input sources, various transformations and calculations and various output targets.
It provides various options to load data from source like a relational database, file, excel sheet, web url and many more.

**Kitchen** is a service to run jobs, you can run the service from windows as well as linux it comes with both batch and sh files

**Pan** is a service to run transformations, you can run the service from windows as well as linux it comes with both batch and sh files.


PDI is developed on java so the memory optimisation can be done with permgen space setting in Spoon.

PDI comes with number of relational database support such as mysql, oracle, ms sql server and many more.

You can use PDI for various use cases such as,

**Moving data from one database to another,** If you have two different databases like mysql and Microsoft SQL then also it is very easy to write a simple transformation and data movement can be done very easily.

**Preparing the ODS (Operational Data storage) tables,** these tables usually contains operational data to generate reports and for other purposes. ODS tables are usually simplified tables in which you store the data extracted from main database, where the data is spread across multiple tables.

**Data warehouse creation,** In data warehouse you will store all the modifications made on a single row with appropriate versioning. PDI provides simplified options to create and maintain a data warehouse tables.It's Dimensional lookup/Update functionality can be specifically used for data warehouse like storage solution.

**For Streaming analytics,** PDI supports near real time streaming analytics you can process log files generated by various applications, or you can configure various consumers like AMQP, MQTT, Kafka, JMS and Kinesis.


Although PDI is a very easy to use and a dynamic tool with various databases, file types and web protocols support, It still requires careful analysis on memory management while developing the job.

You have to tune your transformations based on estimated data loads otherwise PDI will easily throw out of memory error. 
Also on the internet there is not much community or forums to look for the issues you face during the development which makes it very difficult to solve the problems.

But once the tool is mastered it does wonders in processing large volume of data.